{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir=\"~/Desktop/biology/breast_cancer/data/\"\n",
    "# ubunto:\n",
    "# old samples\n",
    "dir1=\"~/Dropbox/CyTOF_Breast/Kaplan_1st/\"\n",
    "# new samples\n",
    "dir2 = '~/Dropbox/CyTOF_Breast/Kaplan_2nd/data_afterGating/processed data/'\n",
    "dir3 = '/home/yishai/Dropbox/CyTOF_Breast/Kaplan_3rd/data/'\n",
    "dir4 = '/home/yishai/Dropbox/CyTOF_Breast/Kaplan_4th/data/'\n",
    "dir5 = '/home/yishai/Dropbox/CyTOF_Breast/Kaplan_5th/data/'\n",
    "\n",
    "\n",
    "# show figures (figures are created and saved to file)\n",
    "show = False\n",
    "# create plot visualizations\n",
    "visualize = False\n",
    "# subsample = True\n",
    "HistFit = False\n",
    "\n",
    "# plt.savefig(\"test.svg\", format=\"svg\")\n",
    "saveSVG = False\n",
    "\n",
    "adjusted = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "envirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:15:54.700552Z",
     "start_time": "2022-12-11T11:15:54.680091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yishai/Data/_params.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import time \n",
    "start = time.process_time()\n",
    "import os\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "import datetime \n",
    "date = datetime.date.today().strftime(\"%d%m%Y\")\n",
    "\n",
    "import sys\n",
    "from IPython.display import Image, display\n",
    "# from tqdm import tqdm_notebook,tqdm\n",
    "# import time\n",
    "\n",
    "import numpy as np\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rc_context\n",
    "# import matplotlib.patches as  mpatches\n",
    "plt.rcParams[\"figure.figsize\"] = (5.0, 4.0)  # Set default size of plots.\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 180) #according to screen width\n",
    "# from pandas.core.base import PandasObject\n",
    "# PandasObject.view = view #allows view meth pd.view\n",
    "\n",
    "# from scipy import signal, stats\n",
    "import seaborn as sns\n",
    "\n",
    "# from sklearn import metrics\n",
    "# from sklearn.cluster import KMeans, DBSCAN, MiniBatchKMeans\n",
    "# from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "# from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "# from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from lmfit import minimize, Parameters\n",
    "# from umap import UMAP\n",
    "\n",
    "# import scanpy as sc\n",
    "# import anndata\n",
    "\n",
    "# import schist as scs\n",
    "\n",
    "# import networkx as nx\n",
    "\n",
    "# from castle.common import GraphDAG\n",
    "# from castle.metrics import MetricsDAG\n",
    "# from castle.datasets import IIDSimulation, DAG\n",
    "# from castle.algorithms import PC,Notears,GOLEM,ANMNonlinear,DirectLiNGAM,ICALiNGAM,NotearsLowRank\n",
    "# import notears.notears as notears\n",
    "\n",
    "# from shapely.geometry import Point\n",
    "# from shapely.geometry.polygon import Polygon\n",
    "\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# from statsmodels.graphics._regressionplots_doc import _plot_influence_doc\n",
    "# from statsmodels.regression.linear_model import OLS\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from pandas import MultiIndex, Int16Dtype\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir+'/functions/')\n",
    "from plot_functions import *\n",
    "from usefull_functions import *\n",
    "from preprocess_functions import *\n",
    "from functions import *\n",
    "from impute_functions import *\n",
    "# from normalization import *\n",
    "from equalize_distributions import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "dir_data = os.path.abspath(os.path.join(parent_dir, os.pardir))+'/Data/'\n",
    "dir_indexes = parent_dir + '/indexes/'\n",
    "# dir_indexes = os.path.abspath(os.path.join(parent_dir, os.pardir))+'/indexes/'\n",
    "\n",
    "# plot dir\n",
    "dir_plots = os.path.abspath(os.path.join(parent_dir, os.pardir))+f'/_preproccess_{date}/'\n",
    "\n",
    "# dir_plots = parent_dir+'/Plots_preproccess/'\n",
    "settings =        (dir_plots,show,saveSVG)\n",
    "\n",
    "folderExists(dir_plots)\n",
    "folderExists(dir_data)\n",
    "folderExists(dir_indexes)\n",
    "\n",
    "import shutil\n",
    "shutil.copy('_params.csv', dir_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature names (Participating panel)\n",
    "\n",
    "Received CyTOF data is assumed to be only of intact and alive cells without duplicates \n",
    "\n",
    "each sample matrix with high dimensionality feature space (NamesAll);\n",
    "1. 1st cytof batch without: 'NCad','ECad','panKeratin'\n",
    "2. 2nd cytof batch without: 'p53', 'ZEB1'\n",
    "3. 3rd cytof batch without: 'p53', 'ZEB1'\n",
    "\n",
    "un-needed features (removed from  future processing); \n",
    "1. 'Live_Dead' (just noise amplification)\n",
    "2. 'Ir_DNA2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:15:54.721783Z",
     "start_time": "2022-12-11T11:15:54.701446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features are correct!\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "features['NamesAll'] = [  \n",
    "            'CD45','H4','H3', 'H3.3','NCad','ECad','panKeratin', 'K5', 'EpCam', 'H3K27me2', 'p53', 'EZH2',  \n",
    "            'gH2AX','aSMA','H3K36me2','H3K4me1','H3K9me2','H4K16ac',\n",
    "            'H2Aub', 'Vimentin', 'H3K64ac', 'BMI-1', 'ZEB1',  'H3K27ac', 'H4K20me3', 'ER', 'CD49f', 'CD24',\n",
    "            'GATA3',  'H3K9ac', 'H3K9me3', 'CD44', 'Ki67', 'K8-18',  'Ir_DNA2', 'Live_Dead',\n",
    "            'H3K36me3','H3K4me3','H3K27me3', 'MBD', \n",
    "            'CyclinB1', 'pRB','H3S28p'\n",
    "        ]\n",
    "features['Core'] = ['H4','H3', 'H3.3']\n",
    "# extra cellular are outside the cell - except them everything is noramalized (in group \"ToNorm\")\n",
    "features['ToNorm'] = [    \n",
    "            'H4','H3', 'H3.3','panKeratin', 'K5', 'H3K27me2', 'p53', 'EZH2', 'gH2AX', 'aSMA', 'H3K36me2', 'H3K4me1', 'H3K9me2',\n",
    "            'H4K16ac', 'H2Aub', 'Vimentin', 'H3K64ac', 'BMI-1', 'ZEB1', 'H3K27ac', 'H4K20me3',\n",
    "            'ER', 'GATA3', 'H3K9ac', 'H3K9me3', 'Ki67', 'K8-18', 'H3S28p', 'H3K36me3','H3K4me3','H3K27me3',  'MBD', 'CyclinB1', 'pRB'\n",
    "        ]\n",
    "features['CellIden'] = [  \n",
    "            'CD45', 'K5', 'EpCam', 'aSMA', 'Vimentin', 'ZEB1', 'ER', 'CD49f', 'CD24', 'GATA3', 'CD44', 'K8-18',\n",
    "            'Ki67','p53', 'NCad','ECad','panKeratin','CyclinB1', 'pRB'\n",
    "        ]\n",
    "# everything that is not cell identity; 'H4','H3', 'H3.3' are excluded\n",
    "features['EpiCols'] = [   \n",
    "            'H3K27me2', 'H3K36me2', 'H3K4me1', 'H3K9me2', 'H4K16ac', 'H2Aub', 'H3K64ac',  \n",
    "            'H3K27ac', 'H4K20me3', 'BMI-1','EZH2','gH2AX',\n",
    "            'H3K9ac', 'H3K9me3', 'H3S28p','H3K36me3', 'H3K4me3','H3K27me3','MBD'\n",
    "        ]\n",
    "# remove un-needed features\n",
    "features = removeFeatures(features.copy(),remove_features =[ 'Live_Dead','Ir_DNA2'])\n",
    "# verify: no feature is both in CellIden and EpiCols (CellIden +EpiCols = NamesAll) \n",
    "if test_fetures(features): print (f'features are correct!') \n",
    "else:    print ('error')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from csv, and store in dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:15:56.012810Z",
     "start_time": "2022-12-11T11:15:54.722716Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dictionarys for data and used feature in each sample:\n",
    "k={}; names = {}\n",
    "names['figures'] = {#'1':1,'2':1,\n",
    "                    '3':2,'4':2,'5':2,\n",
    "                        '4.1':3,'7':3,'8':3,'11':3, \n",
    "                                '7.1':4,\"13\":4,'14':4,\"15\":4,\n",
    "                                    '8.1':1,'17':1,'18':1,'19':1,'20':1,\n",
    "\n",
    "                    \"14.2\":5,'18.2':5# withot refernce to rest\n",
    "                    }\n",
    "# names['norm_type'] = {  '1':1,'2':1,  \n",
    "#                         '3':2,'4':2,'5':2,    \n",
    "#                         '4.1':2,'7':2,'8':2,'11':2,  \n",
    "#                         '7.1':2,\"13\":2,'14':2,\"15\":2, \"14.2\":2,  \n",
    "#                         # \"14.4\":2,\n",
    "#                         '8.1':2,'17':2,'18':2,'19':2,'20':2, '18.2':2,\n",
    "                      \n",
    "#                         '7.3':2,'13.3':2,'14.3':2,'15.3':2,\n",
    "#                         '8.3':2,'17.3':2,'18.3':2,'19.3':2,'20.3':2,\n",
    "#                       }\n",
    "# 'b1':1,'b2':2, 'b3':2,'b4':2,'b23':2,'b234':2,\n",
    "names['all'] = features\n",
    "\n",
    "\n",
    "# 1st cytof batch\n",
    "# features diffrence of 1st cytof batch - copy into names dict\n",
    "# f = removeFeatures(features.copy(),remove_features =['NCad','ECad','panKeratin','MBD', 'CyclinB1', 'pRB',])\n",
    "# names['1'],names['2']  = f,f\n",
    "# # copy csv data into data dict according to used features dict (names)\n",
    "# k['1'] = pd.read_csv(dir1+\"BCK-01_noaf_18Sep2022_01_0.fcs_file_internal_comp_residual.csv\")[names['1']['NamesAll']]\n",
    "# k['2'] = pd.read_csv(dir1+\"BCK-02_noaf_18Sep2022_01_0.fcs_file_internal_comp_residual.csv\")[names['2']['NamesAll']]\n",
    "\n",
    "# 2nd cytof batch\n",
    "# features diffrence of 2nd cytof batch - copy into names dict\n",
    "f = removeFeatures(features.copy(),remove_features =['p53','ZEB1','MBD', 'CyclinB1', 'pRB'])\n",
    "names['3'],names['4'],names['5']  = f,f,f\n",
    "# k['3'] = pd.read_csv(dir2+\"export_BCK03_noaf_23Nov2022_01_0_final_cells - processed.csv\")[names['3']['NamesAll']]\n",
    "k['4'] = pd.read_csv(dir2+\"export_BCK04_noaf_23Nov2022_02_0_final_cells - processed.csv\")[names['4']['NamesAll']]\n",
    "k['5'] = pd.read_csv(dir2+\"export_BCK05_noaf_23Nov2022_01_0_final_cells - processed.csv\")[names['5']['NamesAll']]\n",
    "# later analysis k6 with duplicates\n",
    "# K6=pd.read_csv(dir2+\"export_BCK05_noaf_23Nov2022_01_0_final_cells.csv\")\n",
    "\n",
    "# 3d cytof batch ADDED H3K4me33\n",
    "# features diffrence of 3d cytof batch - copy into names dict\n",
    "f = removeFeatures(features.copy(),remove_features =['p53','ZEB1','MBD', 'CyclinB1', 'pRB', 'NCad'])\n",
    "\n",
    "names['4.1'],names['7'],names['8'],names['11']  = f,f,f,f\n",
    "k['4.1'] = pd.read_csv(dir3+\"export_BCK04_noaf_16Mar2023_03_0_FinalCells.csv\")[names['4.1']['NamesAll']]  # with 'NCad' ' without , 'MBD\n",
    "k['7'] = pd.read_csv(dir3+\"export_BCK07_noaf_16Mar2023_03_0_FinalCells.csv\")[names['7']['NamesAll']] # with 'NCad' ' without , 'MBD\n",
    "k['8'] = pd.read_csv(dir3+\"export_BCK08_noaf_16Mar2023_02_0_FinalCells.csv\")[names['8']['NamesAll']] # with 'NCad' ' without , 'MBD\n",
    "k['11'] = pd.read_csv(dir3+\"export_BCK11_noaf_16Mar2023_01_0_FinalCells.csv\")[names['11']['NamesAll']] # with 'NCad' ' without , 'MBD\n",
    "# ----------------------------------------------------------------------\n",
    "# 4th cytof batch ADDED H3K4me33\n",
    "f = removeFeatures(features.copy(),remove_features =['p53','ZEB1','CyclinB1','pRB','NCad',   ])\n",
    "names['7.1'],names['13'],names['14'],names['15']  = f,f,f,f\n",
    "k['7.1'] = pd.read_csv(dir4+\"BCK7.1_FinalCells.csv\")[names['7.1']['NamesAll']] # with both 'NCad' and 'MBD' in 'MBD' ('Ncad' is low can be assuamed as 'MBD)\n",
    "k['13'] = pd.read_csv(dir4+\"BCK13_FinalCells.csv\")[names['13']['NamesAll']] # with 'MBD' without ,'NCad'\n",
    "k['14'] = pd.read_csv(dir4+\"BCK14_FinalCells.csv\")[names['14']['NamesAll']] # with 'MBD' without ,'NCad'\n",
    "k['15'] = pd.read_csv(dir4+\"BCK15_FinalCells.csv\")[names['15']['NamesAll']] # with 'MBD' without ,'NCad'\n",
    "\n",
    "# celcycle 4th cytof batch - without reference to rest\n",
    "# names['14.4'] =removeFeatures(features.copy(),remove_features =['p53','ZEB1','NCad', 'CyclinB1', 'pRB', ]);\n",
    "# names['14.4']['NamesAll'].append('DNA2')\n",
    "# k['14.4'] = pd.read_csv(dir4+\"BCK14_FinalCells.csv\")[names['14.4']['NamesAll']]\n",
    "# celcycle\n",
    "names['14.2'] = removeFeatures(features.copy(),remove_features =['p53','ZEB1', 'NCad',     'H3K4me1', 'H2Aub', 'MBD', ])\n",
    "names['14.2']['NamesAll'].append('DNA2')\n",
    "k['14.2'] = pd.read_csv(dir4+\"BCK14.2_FinalCells.csv\")[names['14.2']['NamesAll']]\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5th cytof batch \n",
    "f = removeFeatures(features.copy(),remove_features =[ 'p53', 'ZEB1', 'CyclinB1', 'pRB' , 'NCad'  ])\n",
    "\n",
    "names['17'],names['18'],names['19'],names['20']  = f,f,f,f\n",
    "k['17'] = pd.read_csv(dir5+\"BCK17_FinalCells.csv\")  [names['17']['NamesAll']] # with 'MBD' without ,'NCad'\n",
    "k['18'] = pd.read_csv(dir5+\"BCK18_FinalCells.csv\")  [names['18']['NamesAll']] # with 'MBD' without ,'NCad'\n",
    "k['19'] = pd.read_csv(dir5+\"BCK19_FinalCells.csv\")  [names['19']['NamesAll']] # with 'MBD' without ,'NCad'\n",
    "k['20'] = pd.read_csv(dir5+\"BCK20_FinalCells.csv\")  [names['20']['NamesAll']] # with 'MBD' without ,'NCad'\n",
    "\n",
    "names['8.1'] = removeFeatures(features.copy(),remove_features =[ 'p53', 'ZEB1', 'CyclinB1', 'pRB' ,'MBD'])\n",
    "k['8.1'] = pd.read_csv(dir5+\"BCK8.1_FinalCells.csv\")[names['8.1']['NamesAll']]# with ,'NCad' without 'MBD' - important MBD FIT IS IMPOSIBLE\n",
    "\n",
    "\n",
    "# celcycle \n",
    "names['18.2'] = removeFeatures(features.copy(),remove_features =['NCad', 'p53', 'H3K4me1', 'H2Aub', 'ZEB1'])\n",
    "names['18.2']['NamesAll'].append('DNA2')\n",
    "\n",
    "k['18.2'] = pd.read_csv(dir5+\"BCK18.2_CellCycle_FinalCells.csv\") [names['18.2']['NamesAll']]\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "for i in k.keys():\n",
    "    k[i]['samp'] = float(i)\n",
    "    k[i]['ind'] = k[i].index\n",
    "\n",
    "    # names[i]['by_sample'] = k[i]['by_sample'].copy() \n",
    "    # names[i]['Ind'] = k[i]['Ind'].copy() \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# gating and oulier removal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gating - fix fluidigm gating result; In-gate and outlier removal (Samples with low number of events)\n",
    "\n",
    "Samples with low number of events are discarded (outliers) - remove outlier 99.99% from all\n",
    "\n",
    "In gate to achieve good normalization - gate on H3.3/H4 was too low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:15:57.286122Z",
     "start_time": "2022-12-11T11:15:56.691586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k4: gated with method 2 , initial size: 401141, after gating and outliers: 374890 (93.46%)\n",
      "k5: gated with method 2 , initial size: 79376, after gating and outliers: 68765 (86.63%)\n",
      "k4.1: gated with method 2 , initial size: 111614, after gating and outliers: 110472 (98.98%)\n",
      "k7: gated with method 2 , initial size: 70379, after gating and outliers: 66748 (94.84%)\n",
      "k8: gated with method 2 , initial size: 130074, after gating and outliers: 124737 (95.9%)\n",
      "k11: gated with method 2 , initial size: 41305, after gating and outliers: 38868 (94.1%)\n",
      "k7.1: gated with method 2 , initial size: 53571, after gating and outliers: 43418 (81.05%)\n",
      "k13: gated with method 2 , initial size: 27493, after gating and outliers: 24167 (87.9%)\n",
      "k14: gated with method 2 , initial size: 74046, after gating and outliers: 58303 (78.74%)\n",
      "k15: gated with method 2 , initial size: 69778, after gating and outliers: 40010 (57.34%)\n",
      "k14.2: gated with method 2 , initial size: 49819, after gating and outliers: 31953 (64.14%)\n",
      "k17: gated with method 2 , initial size: 45667, after gating and outliers: 36988 (81.0%)\n",
      "k18: gated with method 2 , initial size: 52250, after gating and outliers: 44601 (85.36%)\n",
      "k19: gated with method 2 , initial size: 84401, after gating and outliers: 63345 (75.05%)\n",
      "k20: gated with method 2 , initial size: 81855, after gating and outliers: 68763 (84.01%)\n",
      "k8.1: gated with method 2 , initial size: 145499, after gating and outliers: 122182 (83.97%)\n",
      "k18.2: gated with method 2 , initial size: 47613, after gating and outliers: 41179 (86.49%)\n"
     ]
    }
   ],
   "source": [
    "arr=[]\n",
    "# GateColumns=['H3.3','H4']#,'H3']#,'H3']\n",
    "for i, K ,in k.items():\n",
    "    norm_type =2 if i not in ['1','2'] else 1\n",
    "    coreFetures = ['H4','H3', 'H3.3'] if norm_type==2 else ['H3.3','H4']\n",
    "    cols = names[i]['NamesAll']\n",
    "\n",
    "    initialSize = len(K)\n",
    "    K,tmp = Gate(K, i,cols,coreFetures)\n",
    "\n",
    "    finalSize = len(K)\n",
    "    print (f'k{i}: gated with method {norm_type} , initial size: {initialSize}, after gating and outliers: {finalSize} ({np.round(finalSize/initialSize*100,2)}%)')\n",
    "    # k[i] = K.reset_index(drop=True)\n",
    "    k[i] = K\n",
    "\n",
    "    arr += tmp\n",
    "\n",
    "saveCsv_split(dir_plots,'gate',arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save to file .2 samples raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2a_indexes ; loaded from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size: 31953, new size: 4489 indexes loaded from file\n",
      "k14.2a; samples =  4489 , fetures =  38 ; saved to file\n",
      "18.2a_indexes ; loaded from file\n",
      "original size: 41179, new size: 4549 indexes loaded from file\n",
      "k18.2a; samples =  4549 , fetures =  39 ; saved to file\n"
     ]
    }
   ],
   "source": [
    "type_ = 'raw'\n",
    "keys = [i for i in [i for i in k.keys() if '.2' in i] if os.path.exists(f\"{dir_indexes}{f'{i}a'}_indexes.p\")]\n",
    "for i in keys:\n",
    "    i_adj = f'{i}a'\n",
    "    # if os.path.exists(f\"{dir_indexes}{i_adj}_indexes.p\"):\n",
    "    K = k[i].copy()\n",
    "    # print(i_adj)\n",
    "    # k[i_adj]= subsample_k(k[i].copy(),i_adj,dir_indexes)\n",
    "    names_ = names[i].copy()\n",
    "    idx = pickle_load(f\"{i_adj}_indexes\",dir_indexes)\n",
    "    \n",
    "    newIdx = [ K.index[K['ind']==i][0] for i in idx ]\n",
    "    K=K.loc[newIdx].copy()\n",
    "    print (f'original size: {len(k[i])}, new size: {len(K)} indexes loaded from file')\n",
    "\n",
    "    dict = None;dict ={}\n",
    "    dict = names_\n",
    "    pickle_dump(f'k_{type_}{i_adj}_names_dict', dict,dir_data)\n",
    "\n",
    "    dict = None;dict ={}\n",
    "    dict['k'] = K\n",
    "    pickle_dump(f'k_{type_}{i_adj}_dict', dict,dir_data)\n",
    "    \n",
    "    print (f'k{i_adj}; samples = ', len(dict['k']), ', fetures = ', len(dict['k'].columns),'; saved to file')\n",
    "    del names_; del K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if visualize:\n",
    "#   interst=[  'CD45','CD44', 'EpCam', 'Vimentin', 'CD49f','CD45','H4','H3', 'H3.3','NCad','ECad','panKeratin',]\n",
    "    interst=names['all']['NamesAll']\n",
    "    \n",
    "\n",
    "\n",
    "    plot_hist(k,interst,names['figures'],settings,\n",
    "            func = sns.kdeplot,title = 'Raw before qqFit',\n",
    "            Figname = '0_Raw_before_qqFit_' \n",
    "            ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['4', '5', '4.1', '7', '8', '11', '7.1', '13', '14', '15', '14.2', '17', '18', '19', '20', '8.1', '18.2'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.keys()\n",
    "\n",
    "\n",
    "# add sample.3 for samples with with specific features which would be dropped at the hist fit\n",
    "# for i in ['7.1','13','14','15','8.1','17','18','19','20']:\n",
    "#     newInd = f'{int(float(i))}.3'\n",
    "#     # print(newInd)\n",
    "#     k[newInd] = k[i].copy()\n",
    "#     names[newInd] = names[i].copy()\n",
    "#     # k[newInd]['samp']  = float(newInd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit histogram (qqfit\\linear fit)' MixedFit,qqFit,LinearFit\n",
    "we fit from 4 into 4.1 (samples 4,5) and from 7.1 into 7 \n",
    "\n",
    "only the fitted columns are modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in ['7.1','13','14','15']:\n",
    "#     newInd = f'{int(float(i))}.3'\n",
    "#     # print(newInd)\n",
    "#     k[newInd] = k[i].copy()\n",
    "#     names[newInd] = names[i].copy()\n",
    "#     k[newInd]['samp']  = float(newInd)\n",
    "\n",
    "\n",
    "# batches to fit (spaces:0,1,2), used also to load fit already saved in file\n",
    "to_fit = {  '4':0,'5':0,#3 is not fitted\n",
    "            '7.1':1,'13':1,'14':1,'15':1, #14.2 is not fitted   \n",
    "            '8.1':2,'17':2,'18':2,'19':2,'20':2, #18.2 is not fitted   \n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if HistFit:\n",
    "    arr_ =[]\n",
    "    keepCols = ['ind','samp']\n",
    "    fit_from, fit_into,fit_cols_,dropped_cols_ = fit_info(k,dir_indexes,keepCols = keepCols)\n",
    "\n",
    "    for i,fit_ind in to_fit.items():\n",
    "        # if int(float(i))<17 :\n",
    "        #     continue\n",
    "        K = k[i].copy()\n",
    "\n",
    "        fit_cols = fit_cols_[fit_ind].copy()\n",
    "        # dropped_cols1 = [col for col in K.columns if col not in cols_ + keepCols] + [col for col in  cols_ + keepCols if col not in K.columns]\n",
    "        dropped_cols = dropped_cols_[fit_ind] + [col for col in  K.columns if col not in fit_cols+keepCols]#cols not in anchor are dropped\n",
    "        dropped_cols = [col for col in dropped_cols if col in K.columns] #remove cols not in K\n",
    "        dropped_cols = list(dict.fromkeys(dropped_cols ))#remove duplicates\n",
    "        print(f'samp{i}, fit_ind:{fit_ind}, cols to drop: {dropped_cols}')\n",
    "        # ---------------------------------\n",
    "        \n",
    "        K.drop(dropped_cols,axis = 1,inplace = True)\n",
    "\n",
    "        K[fit_cols] =  fitDF(fit_into[fit_ind].copy(),fit_from[fit_ind].copy(),\n",
    "            df_to_fit = K[fit_cols].copy(),func = MixedFit)\n",
    "        arr_.append(f'k{i}:{dropped_cols}')\n",
    "        j = f'{i}f'#fit index\n",
    "        k[j] = K.copy();K = None\n",
    "        names[j] = removeFeatures(names[i].copy(),dropped_cols)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names_dict ; loaded from file\n",
      "k_fit4f_dict ; loaded from file\n",
      "k_fit5f_dict ; loaded from file\n",
      "k_fit7.1f_dict ; loaded from file\n",
      "k_fit13f_dict ; loaded from file\n",
      "k_fit14f_dict ; loaded from file\n",
      "k_fit15f_dict ; loaded from file\n",
      "k_fit8.1f_dict ; loaded from file\n",
      "k_fit17f_dict ; loaded from file\n",
      "k_fit18f_dict ; loaded from file\n",
      "k_fit19f_dict ; loaded from file\n",
      "k_fit20f_dict ; loaded from file\n",
      "['4', '5', '4.1', '7', '8', '11', '7.1', '13', '14', '15', '14.2', '17', '18', '19', '20', '8.1', '18.2', '4f', '5f', '7.1f', '13f', '14f', '15f', '8.1f', '17f', '18f', '19f', '20f', '4.1f', '7f', '8f', '11f']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if HistFit:\n",
    "    arr_ =[]\n",
    "    c = None\n",
    "    for i,_ in to_fit.items():\n",
    " \n",
    "        # unit check: verify all samples with same columns\n",
    "        c = list(k[f'{i}f'].columns.copy()) if c is None else c #take first sample featurs as reference\n",
    "        print(i, 'same columns:',all(c==k[f'{i}f'].columns))\n",
    "    print(f'fitted with dropped cols: {arr_} ') ;\n",
    "    arr_ = []\n",
    "    for i,fit_ind in to_fit.items():\n",
    "        i = f'{i}f'#fit index\n",
    "        K = k[i].copy() #only samples which are fitted are saved as fit for load confortibilty\n",
    "        dict = None;dict ={}\n",
    "        dict['k'] = K.copy()\n",
    "        pickle_dump('k_fit'+i+'_dict', dict,dir_data)\n",
    "\n",
    "        dict = None;dict ={}\n",
    "        dict = names[i].copy()\n",
    "        pickle_dump('k'+i+'_names_dict', dict,dir_data)\n",
    "\n",
    "        \n",
    "        arr_.append(f'k{i}:len{len(K)}')\n",
    "    pickle_dump('names_dict', names.copy(),dir_data)\n",
    "\n",
    "    print(f'saved to file: {arr_} ') ;arr_ = None\n",
    "\n",
    "\n",
    "if not HistFit: #load from file fitted samples\n",
    "    names = pickle_load('names_dict', dir_data)\n",
    "    kk = {}\n",
    "    for i in to_fit.keys():# only samples which are fitted are saved in this dict\n",
    "        i = f'{i}f'#fit index\n",
    "        k[i] = pickle_load(f'k_fit{i}_dict', dir_data)['k']\n",
    "#space fitted  is not fitted but is relative to fitted samples\n",
    "# we duplicate it also to the fitted samples space\n",
    "for i in ['4.1', '7', '8', '11']:\n",
    "    k[f'{i}f'] = k[i].copy()\n",
    "    names[f'{i}f'] = names[i].copy()\n",
    "print(list(k.keys() )) \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# impute\n",
    "\n",
    "impute EpiCols with the right dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['4', '5', '4.1', '7', '8', '11', '7.1', '13', '14', '15', '14.2', '17', '18', '19', '20', '8.1', '18.2', '4f', '5f', '7.1f', '13f', '14f', '15f', '8.1f', '17f', '18f', '19f', '20f', '4.1f', '7f', '8f', '11f'])\n"
     ]
    }
   ],
   "source": [
    "print(k.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n = names['all']['EpiCols'].copy()\n",
    "# remove cropped distrunutions\n",
    "# for f in ['H3K4me1','H3K4me3','H3K27me2','H3K36me2','H3S28p']:\n",
    "#     n.remove(f)\n",
    "# zeros = getZerosMat(k,index=['1','2','3','4','5'],columns = n)\n",
    "\n",
    "# zeros = getZerosMat(k,columns = names['all']['NamesAll'].copy(),percent = 0)\n",
    "# zeros.to_csv(settings[0]+'raw_zeros.csv')\n",
    "# print(zeros)\n",
    "# # m=\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_hist(k,NamesAll,figures,settings,func = sns.kdeplot ,title = '',Figname = '' ):\n",
    "    \n",
    "#     arr = np.linspace(0, 1, len (k.keys()))    \n",
    "#     colors = cm.rainbow(arr)\n",
    "#     for M in NamesAll: \n",
    "#         fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "        \n",
    "#         for [i, K],color,fig_num in zip(k.items(),colors,figures):\n",
    "#             fig_num -= 1\n",
    "#             ax[fig_num].set_ylim(0,5000)\n",
    "#             ax[fig_num].set_xlim(0,500)\n",
    "\n",
    "#             # sns.kdeplot(K[M],c=c,label='Tumor ' + i)\n",
    "#             try: #if K doesnt contain the feature pass..\n",
    "#               q=func(K[M],color=color,label='Tumor ' + i,ax = ax[fig_num])\n",
    "#               # sns.kdeplot(K2[M],c='g',label='Tumor 2')\n",
    "#               ax[fig_num].title.set_text(title)\n",
    "#               ax[fig_num].legend()\n",
    "              \n",
    "              \n",
    "#             except:\n",
    "#               pass\n",
    "        \n",
    "#         figname = Figname + M\n",
    "        \n",
    "#         dir,show,saveSVG = settings\n",
    "#         plt.savefig(dir+figname+'.png', format=\"png\", bbox_inches=\"tight\", pad_inches=0.2)\n",
    "#         if saveSVG:\n",
    "#             plt.savefig(dir+figname+'.svg', format=\"svg\", bbox_inches=\"tight\", pad_inches=0.2)\n",
    "#         if show:\n",
    "#             plt.show()\n",
    "#         else:\n",
    "#             plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i='1'\n",
    "# n=zeros.T[zeros.T['1'].notnull()].index.tolist()\n",
    "\n",
    "# nonzeros = MimalzerosOverlap(k[i].copy(),names[i]['Core']+names[i]['CellIden'],n)\n",
    "# features = nonzeros.columns\n",
    "# K = k[i].copy()\n",
    "# # features=['p53']\n",
    "# df = pd.DataFrame(index=['mse','mae','accuracy'],columns = features)\n",
    "\n",
    "# for feature in features:\n",
    "\n",
    "#     K,mse, mae, accuracy = xg(nonzeros,K,feature)\n",
    "#     df.at['mse',feature] = mse\n",
    "#     df.at['mae',feature] = mae\n",
    "#     df.at['accuracy',feature] = accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(zeros.T[zeros.T['1'].notnull()].index.tolist())\n",
    "# print(df.T.index)\n",
    "# print(df.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arcsinh data transformation (semi log scaling):\n",
    "  \n",
    "  1. find ki67 neg using scale argument = 0.1\n",
    "  2. actual data transformation with scale argument = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find ki67 - not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from plot_functions import *\n",
    "# arr = []\n",
    "# f = 'Ki67'\n",
    "# kk = k.copy()\n",
    "# min_x,min_y,Ki67Neg_ind ={},{},{}\n",
    "# for [i, K] in kk.items():\n",
    "#     # K = K['Ki67'].copy()\n",
    "#     # m = K['Ki67'].max()\n",
    "#     K = arcsinh_transform(K.copy(),scale = 0.1)\n",
    "#     # print (f'k{i}; Ki67 arcsinh transformed - {scale}')\n",
    "#     min_x[i],min_y[i] = splitInversePDF(K,i,f)\n",
    "#     kk[i] = K\n",
    "#     # kk[i] = K\n",
    "#     Ki67Neg_ind[i]= K[f]<min_x[i]\n",
    "#     # print(f'k{i}; value with highest *inverse* probability:', min_x[i])\n",
    "#     neg_percentage = np.round(len(K[K[f]<min_x[i]])/len(K),3)*100\n",
    "#     arr.append([i,f,neg_percentage])\n",
    "#     plotSplit(K[f],i,min_x[i],min_y[i],settings,neg_percentage,Figname = f'_{f}_split',log = False)\n",
    "# saveCsv_split(dir_plots,'neg_pecentage',arr)\n",
    "# kk = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:15:57.623862Z",
     "start_time": "2022-12-11T11:15:57.287245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcsinh transformed; ['k4', 'k5', 'k4.1', 'k7', 'k8', 'k11', 'k7.1', 'k13', 'k14', 'k15', 'k14.2', 'k17', 'k18', 'k19', 'k20', 'k8.1', 'k18.2', 'k4f', 'k5f', 'k7.1f', 'k13f', 'k14f', 'k15f', 'k8.1f', 'k17f', 'k18f', 'k19f', 'k20f', 'k4.1f', 'k7f', 'k8f', 'k11f']\n"
     ]
    }
   ],
   "source": [
    "arr_=[]\n",
    "for i, K in k.items():\n",
    "    cols = names[i]['NamesAll']\n",
    "    k[i][cols] = arcsinh_transform(K.copy(),cols)\n",
    "    arr_.append(f'k{i}')\n",
    "print (f'arcsinh transformed; {arr_}');arr_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save to file .2 samples arcsinh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2a_indexes ; loaded from file\n",
      "original size: 31953, new size: 4489 indexes loaded from file\n",
      "k14.2a; samples =  4489 , fetures =  38 ; saved to file\n",
      "18.2a_indexes ; loaded from file\n",
      "original size: 41179, new size: 4549 indexes loaded from file\n",
      "k18.2a; samples =  4549 , fetures =  39 ; saved to file\n"
     ]
    }
   ],
   "source": [
    "type_ = 'arcsinh'\n",
    "keys = [i for i in [i for i in k.keys() if '.2' in i] if os.path.exists(f\"{dir_indexes}{f'{i}a'}_indexes.p\")]\n",
    "for i in keys:\n",
    "    i_adj = f'{i}a'\n",
    "    # if os.path.exists(f\"{dir_indexes}{i_adj}_indexes.p\"):\n",
    "    K = k[i].copy()\n",
    "    # print(i_adj)\n",
    "    # k[i_adj]= subsample_k(k[i].copy(),i_adj,dir_indexes)\n",
    "    names_ = names[i].copy()\n",
    "    idx = pickle_load(f\"{i_adj}_indexes\",dir_indexes)\n",
    "    \n",
    "    newIdx = [ K.index[K['ind']==i][0] for i in idx ]\n",
    "    K=K.loc[newIdx].copy()\n",
    "    print (f'original size: {len(k[i])}, new size: {len(K)} indexes loaded from file')\n",
    "\n",
    "    dict = None;dict ={}\n",
    "    dict = names_\n",
    "    pickle_dump(f'k_{type_}{i_adj}_names_dict', dict,dir_data)\n",
    "\n",
    "    dict = None;dict ={}\n",
    "    dict['k'] = K\n",
    "    pickle_dump(f'k_{type_}{i_adj}_dict', dict,dir_data)\n",
    "    \n",
    "    print (f'k{i_adj}; samples = ', len(dict['k']), ', fetures = ', len(dict['k'].columns),'; saved to file')\n",
    "    del names_; del K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization\n",
    "\n",
    "Kernel Distribution Estimation Plot (kdeplot) is the probability density function plot;\n",
    "1. can plot for the univariate (single variable) or multiple variables altogether. \n",
    "2. y = probability, x = feature value after arcsinh transformation (can be also negative)\n",
    "3. arcsinh probability density plot - analysis of herogeneity (division to multiple populations)\n",
    "\n",
    "results:\n",
    "1. cd45 - we want only the negative population\n",
    "2. close populations - epcam, vimentin,cd49f\n",
    "3. far populations - cd44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:16:40.368500Z",
     "start_time": "2022-12-11T11:16:03.415174Z"
    }
   },
   "outputs": [],
   "source": [
    "# from plot_functions import *\n",
    "if visualize:\n",
    "#   interst=[  'CD45','CD44', 'EpCam', 'Vimentin', 'CD49f','CD45','H4','H3', 'H3.3','NCad','ECad','panKeratin',]\n",
    "  interst=names['all']['NamesAll']\n",
    "\n",
    "  plot_hist(k,interst,names['figures'],settings,\n",
    "            func = sns.kdeplot,title = 'ArcSinh Unnormalized',Figname = '1_Hist_ArcSinh_Unnormalized_' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split CD45 neg \n",
    "\n",
    "the only interesting population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# kk=k. copy() \n",
    "# k = kk.copy()\n",
    "# settings=(settings[0],True,True);visualize=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:16:45.618105Z",
     "start_time": "2022-12-11T11:16:40.370107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k14.2; value with highest *inverse* probability: 0.1\n",
      "k18.2; value with highest *inverse* probability: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k4f; value with highest *inverse* probability: 0.06\n",
      "k5f; value with highest *inverse* probability: 1.23\n",
      "k7.1f; value with highest *inverse* probability: 0.1\n",
      "k13f; value with highest *inverse* probability: 0.4\n",
      "k14f; value with highest *inverse* probability: 0.1\n",
      "k15f; value with highest *inverse* probability: 0.1\n",
      "k8.1f; value with highest *inverse* probability: 0.1\n",
      "k17f; value with highest *inverse* probability: 0.3\n",
      "k18f; value with highest *inverse* probability: 0.2\n",
      "k19f; value with highest *inverse* probability: 0.2\n",
      "k20f; value with highest *inverse* probability: 0.1\n",
      "k4.1f; value with highest *inverse* probability: 0.072\n",
      "k7f; value with highest *inverse* probability: 0.11\n",
      "k8f; value with highest *inverse* probability: 0.1\n",
      "k11f; value with highest *inverse* probability: 0.14\n"
     ]
    }
   ],
   "source": [
    "arr=[]\n",
    "KCD45Neg_ind ,min_x, min_y = {},{},{}\n",
    "neg_percentage={}\n",
    "f = 'CD45'\n",
    "\n",
    "xMins = {'1':1.87,'2':2.172,\n",
    "        '3':0.98,'4':0.06,'5':1.23,\n",
    "        '4.1':0.072,'7':0.11,'8':0.10,'11':0.14,\n",
    "\n",
    "        '7.1':0.1,'13':0.4,'14':0.1,'15':0.1,\n",
    "        # '7.3':0.1,'13.3':0.6,'14.3':0.1,'15.3':0.1,\n",
    "\n",
    "        '17':0.3,'18':0.2,'19':0.2,'20':0.1,'8.1':0.1,\n",
    "        # '17.3':0.3,'18.3':0.2,'19.3':0.2,'20.3':0.1,'8.3':0.1,\n",
    "\n",
    "        '14.2':0.1,'18.2':0.1,\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "for j,[i, K] in enumerate(k.items()):\n",
    "    if 'f' not in i and '.2'not in i: #skip unfitted regular samples (we would use fitted index later)\n",
    "        continue\n",
    "    xMin = xMins[ i if 'f' not in i else i[:-1]] \n",
    "\n",
    "    # xMin = xMins[ i]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if xMin == None: #there  are 2 defined dist and minima can be found not manually\n",
    "         min_x[i],min_y[i] = splitInversePDF(K,i,'CD45')\n",
    "    else:\n",
    "        # min_x[i] = xMin\n",
    "        # if visualize:\n",
    "        min_x[i],min_y[i] = splitInversePDF(K,i,f,min_x = xMin)\n",
    "    KCD45Neg_ind[i]= K[K[f]<min_x[i]].index\n",
    "    neg_percentage[i] = np.round(len(K[K[f]<min_x[i]])/len(K),3)*100\n",
    "    arr.append([i,f,neg_percentage[i]])\n",
    "    print(f'k{i}; value with highest *inverse* probability:', min_x[i])\n",
    "saveCsv_split(dir_plots,'neg_pecentage',arr)\n",
    "\n",
    "# visualize = True\n",
    "if visualize:\n",
    "    for [i,f,_]  in arr:\n",
    "    # for i in k.keys():\n",
    "        K = k[i].copy()\n",
    "        # ind = KCD45Neg_ind[i]\n",
    "        # min_x = loc[KCD45Neg_ind[i]].min()\n",
    "        neg_percentage_ = np.round(len(K[K[f]<min_x[i]])/len(K),3)*100\n",
    "       \n",
    "            \n",
    "        plotSplit(k[i][f].copy(),i,min_x[i],min_y[i],settings,neg_percentage_,Figname = '_cd45_split',log = False,)#XSIZE=10\n",
    "       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:20:23.595179Z",
     "start_time": "2022-12-11T11:20:23.218312Z"
    }
   },
   "outputs": [],
   "source": [
    "# if visualize:\n",
    "#   colors = cm.rainbow(np.linspace(0, 1, len (k.keys())))\n",
    "#   fig, ax = plt.subplots(1,2,figsize=(10,3))\n",
    "#   for i, K in k.items(): \n",
    "#     #   \n",
    "#     j =  names['figures'][int(i)-1]-1 \n",
    "#     sns.histplot(K.CD45,color=colors[int(i)-1],label='T'+i,stat='density',element='step',fill=False,ax = ax[j])\n",
    "#     ax[j].legend(loc='upper center',bbox_to_anchor=(1,1))\n",
    "#     ax[j].set_yscale('log')\n",
    "#     ax[j].title.set_text('cd45 split histplot')\n",
    "#   plt.savefig(dir_plots+'cd45_split_histplot.svg', format=\"svg\", bbox_inches=\"tight\", pad_inches=0.2)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#   fig, ax = plt.subplots(1,2,figsize=(12,3))\n",
    "#   for [i, K],[h, KCD45Neg] in zip( k.items(),kCD45Neg.items()):\n",
    "#       j =  names['figures'][int(i)-1]-1 \n",
    "#       sns.kdeplot(K.CD45,color=colors[int(i)-1],label='T'+i,ax = ax[j])\n",
    "#       sns.kdeplot(K.CD45,color=colors[int(i)-1],ls='--',label='T'+i+' CD45-',ax = ax[j])\n",
    "#       ax[j].legend(loc='upper center',bbox_to_anchor=(1,1))\n",
    "#       ax[j].set_yscale('log')\n",
    "#       ax[j].title.set_text('cd45 split kdeplot')\n",
    "#   plt.savefig(dir_plots+'cd45_split_kdeplot.svg', format=\"svg\", bbox_inches=\"tight\", pad_inches=0.2)\n",
    "\n",
    "      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue only with CD45 neg population\n",
    "\n",
    "1. delete cd45+ population and continue with k  = kCD45Neg\n",
    "2. delete cd45 feature from data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k4: CD45+ removal - initial: 374890, final:336286 (89.7%)\n",
      "k5: CD45+ removal - initial: 68765, final:63191 (91.89%)\n",
      "k4.1: CD45+ removal - initial: 110472, final:99433 (90.01%)\n",
      "k7: CD45+ removal - initial: 66748, final:57086 (85.52%)\n",
      "k8: CD45+ removal - initial: 124737, final:108526 (87.0%)\n",
      "k11: CD45+ removal - initial: 38868, final:33054 (85.04%)\n",
      "k7.1: CD45+ removal - initial: 43418, final:37175 (85.62%)\n",
      "k13: CD45+ removal - initial: 24167, final:21674 (89.68%)\n",
      "k14: CD45+ removal - initial: 58303, final:49719 (85.28%)\n",
      "k15: CD45+ removal - initial: 40010, final:35205 (87.99%)\n",
      "k14.2: CD45+ removal - initial: 31953, final:25226 (78.95%)\n",
      "k17: CD45+ removal - initial: 36988, final:32521 (87.92%)\n",
      "k18: CD45+ removal - initial: 44601, final:39253 (88.01%)\n",
      "k19: CD45+ removal - initial: 63345, final:55999 (88.4%)\n",
      "k20: CD45+ removal - initial: 68763, final:59748 (86.89%)\n",
      "k8.1: CD45+ removal - initial: 122182, final:106075 (86.82%)\n",
      "k18.2: CD45+ removal - initial: 41179, final:31220 (75.82%)\n",
      "k4f: CD45+ removal - initial: 374890, final:336286 (89.7%)\n",
      "k5f: CD45+ removal - initial: 68765, final:63191 (91.89%)\n",
      "k7.1f: CD45+ removal - initial: 43418, final:37175 (85.62%)\n",
      "k13f: CD45+ removal - initial: 24167, final:21674 (89.68%)\n",
      "k14f: CD45+ removal - initial: 58303, final:49719 (85.28%)\n",
      "k15f: CD45+ removal - initial: 40010, final:35205 (87.99%)\n",
      "k8.1f: CD45+ removal - initial: 122182, final:106075 (86.82%)\n",
      "k17f: CD45+ removal - initial: 36988, final:32521 (87.92%)\n",
      "k18f: CD45+ removal - initial: 44601, final:39253 (88.01%)\n",
      "k19f: CD45+ removal - initial: 63345, final:55999 (88.4%)\n",
      "k20f: CD45+ removal - initial: 68763, final:59748 (86.89%)\n",
      "k4.1f: CD45+ removal - initial: 110472, final:99433 (90.01%)\n",
      "k7f: CD45+ removal - initial: 66748, final:57086 (85.52%)\n",
      "k8f: CD45+ removal - initial: 124737, final:108526 (87.0%)\n",
      "k11f: CD45+ removal - initial: 38868, final:33054 (85.04%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dict ={}\n",
    "\n",
    "for i, K in k.items():\n",
    "    # if i !='4f':\n",
    "    #     continue\n",
    "    \n",
    "    # if 'f' not in i :\n",
    "    #     continue\n",
    "   \n",
    "    \n",
    "    j = i if ('f' in i) or ('.2'  in i) else i+'f'\n",
    "    # print(i,j)\n",
    "    # print(i,K.loc[KCD45Neg_ind[j]]['CD45'].max())\n",
    "    # j=i\n",
    "    newK = K.loc[KCD45Neg_ind[j]].copy()\\\n",
    "        # .drop('CD45',axis=1) #\\\n",
    "    # .reset_index(drop=True)\n",
    "    # neg_percentage = np.round(len(K[K[f]<min_x[i]])/len(K),3)*100\n",
    "    print(f'k{i}: CD45+ removal - initial: {len(K)}, final:{len(newK)} ({np.round(len(newK)/len(K)*100,2)}%)')\n",
    "    dict[i ] = newK\n",
    "    newK=None\n",
    "    names[i]  = removeFeatures(names[i].copy(),['CD45'])\n",
    "k =None; k=dict \n",
    "names['all']  = removeFeatures(names[i].copy(),['CD45'])\n",
    "# dict ={}\n",
    "# for i, K in k.items():\n",
    "  \n",
    "\n",
    "#   # remove from features lists(names)\n",
    "#   N = names[i]\n",
    "#   for j, sublist in N.items():\n",
    "#     try:\n",
    "#       sublist.remove('CD45')\n",
    "#     except:\n",
    "#       pass\n",
    "#     N[j] = sublist\n",
    "#   dict[i] = N\n",
    "# #   remove index 3 and copy to new dict\n",
    "# # names['figures'].pop(3-1)\n",
    "# dict['figures'] = names['figures']\n",
    "# # names['norm_type'].pop(3-1)\n",
    "# dict['norm_type'] = names['norm_type']\n",
    "# dict['all'] = names['all']\n",
    "# del names; names=dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Normalize using new method on all intercellular markers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save pre normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, K in k.items():\n",
    "#     try:\n",
    "#       del dict;dict ={}\n",
    "#     except:\n",
    "#         dict ={}\n",
    "#     dict['k'] = K\n",
    "#     pickle_dump('k'+i+'_prenorm_dict', dict,dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, K in k.items():\n",
    "#     k[i]['by_sample'] = float(i)\n",
    "#     names[i]['ind'] = k[i]['by_sample']\n",
    "\n",
    "# # merged dataset is normalize with H3 and plot by its own fig\n",
    "# # names['norm_type'].append(2)\n",
    "# # names['figures'].append(4)\n",
    "\n",
    "# # uncommon features between samples and samples index to append\n",
    "\n",
    "# b23 = ['p53', 'ZEB1' ];b23ind = [ '4', '5', '4.1', '7', '8', '11']\n",
    "# b123 = ['NCad','ECad','panKeratin',  'p53', 'ZEB1'];b123ind = ['1', '2', '4', '5', '4.1', '7', '8', '11']\n",
    "# # b4= ['p53', 'ZEB1' ];b4ind = [ '4', '4.1']\n",
    "\n",
    "\n",
    "# # remove the uncommonFeatures from the  mutual list, create data-dict without uncommonFeatures and append into single dataset\n",
    "# names['b123']  = removeFeatures(names['all'].copy(),b123)\n",
    "# k['b123'],names['b123']['ind'] = createAppendDataset(names['b123'],getAppendDict(k.copy(),b123ind,b123 ),n=1771)\n",
    "\n",
    "# names['b23']  = removeFeatures(names['all'].copy(),b23)\n",
    "# k['b23'],names['b23']['ind'] = createAppendDataset(names['b23'],getAppendDict(k.copy(),b23ind,b23 ),n=2000)\n",
    "\n",
    "# # names['b4']  = removeFeatures(names['all'].copy(),b4)\n",
    "# # k['b4'],names['b4']['ind'] = createAppendDataset(names['b4'],getAppendDict(k.copy(),b4ind,b4 ),n=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 step normalization:\n",
    "\n",
    "1- normalize gardient (TBD)\n",
    "\n",
    "1- normalize using core\n",
    "\n",
    "insight: since its the minima multiple rus with same effect as one run\n",
    "\n",
    "note: features to norm (i.e names[i]['ToNorm']) are different on each sample\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # FIRST STEP - normalize_gardient by core \n",
    "        # SECOND STEP - normalize ToNorm fetures by the core features ['H3.3','H4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized with method: ['k4:2', 'k5:2', 'k4.1:2', 'k7:2', 'k8:2', 'k11:2', 'k7.1:2', 'k13:2', 'k14:2', 'k15:2', 'k14.2:2', 'k17:2', 'k18:2', 'k19:2', 'k20:2', 'k8.1:2', 'k18.2:2', 'k4f:2', 'k5f:2', 'k7.1f:2', 'k13f:2', 'k14f:2', 'k15f:2', 'k8.1f:2', 'k17f:2', 'k18f:2', 'k19f:2', 'k20f:2', 'k4.1f:2', 'k7f:2', 'k8f:2', 'k11f:2']\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(k,sample,names):\n",
    "    arr_ =[]\n",
    "    for i, K in k.items():\n",
    "        # i = '7.1';K = k[i]\n",
    "        norm_type = 2 if i not in ['1','2'] else 1\n",
    "        ToNorm = names[i]['ToNorm'].copy()\n",
    "        coreFetures = ['H4','H3', 'H3.3'] if norm_type==2 else ['H3.3','H4']\n",
    "\n",
    "        \n",
    "        if norm_type==1:\n",
    "          K[ToNorm]=NormalizeNew2(K.copy(),ToNorm)\n",
    "          \n",
    "        elif norm_type==2:\n",
    "          K[ToNorm] = NormalizeNew(K.copy(),ToNorm)\n",
    "        K[ToNorm]= Mean_Core_normalization(K.copy(), ToNorm,coreFetures=coreFetures)\n",
    "\n",
    "        if (K.columns != k[i].columns).any():\n",
    "           print('eror')\n",
    "           break\n",
    "        k[i] = K \n",
    "        arr_.append(f'k{i}:{norm_type}')\n",
    "    print (f'normalized with method: {arr_}');arr_ = None\n",
    "    return k\n",
    "\n",
    "k = normalize_data(k.copy(),'k',names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsample original data; verify n<=5000 \n",
    "\n",
    "reset index \n",
    "\n",
    "load both 14 &14.1 from same index file (same indexes)\n",
    "\n",
    "add columns so we would know from which samp any data arrived from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['4', '5', '4.1', '7', '8', '11', '7.1', '13', '14', '15', '14.2', '17', '18', '19', '20', '8.1', '18.2', '4f', '5f', '7.1f', '13f', '14f', '15f', '8.1f', '17f', '18f', '19f', '20f', '4.1f', '7f', '8f', '11f'])\n"
     ]
    }
   ],
   "source": [
    "kk = k.copy()\n",
    "print(k.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_subsample_indexes ; loaded from file\n",
      "original size: 336286, new size: 5000 indexes loaded from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_subsample_indexes ; loaded from file\n",
      "original size: 63191, new size: 5000 indexes loaded from file\n",
      "4.1_subsample_indexes ; loaded from file\n",
      "original size: 99433, new size: 5000 indexes loaded from file\n",
      "7_subsample_indexes ; loaded from file\n",
      "original size: 57086, new size: 5000 indexes loaded from file\n",
      "8_subsample_indexes ; loaded from file\n",
      "original size: 108526, new size: 5000 indexes loaded from file\n",
      "11_subsample_indexes ; loaded from file\n",
      "original size: 33054, new size: 5000 indexes loaded from file\n",
      "7.1_subsample_indexes ; loaded from file\n",
      "original size: 37175, new size: 5000 indexes loaded from file\n",
      "13_subsample_indexes ; loaded from file\n",
      "original size: 21674, new size: 5000 indexes loaded from file\n",
      "14_subsample_indexes ; loaded from file\n",
      "original size: 49719, new size: 5000 indexes loaded from file\n",
      "15_subsample_indexes ; loaded from file\n",
      "original size: 35205, new size: 5000 indexes loaded from file\n",
      "14.2_subsample_indexes ; loaded from file\n",
      "original size: 25226, new size: 5000 indexes loaded from file\n",
      "17_subsample_indexes ; loaded from file\n",
      "original size: 32521, new size: 5000 indexes loaded from file\n",
      "18_subsample_indexes ; loaded from file\n",
      "original size: 39253, new size: 5000 indexes loaded from file\n",
      "19_subsample_indexes ; loaded from file\n",
      "original size: 55999, new size: 5000 indexes loaded from file\n",
      "20_subsample_indexes ; loaded from file\n",
      "original size: 59748, new size: 5000 indexes loaded from file\n",
      "8.1_subsample_indexes ; loaded from file\n",
      "original size: 106075, new size: 5000 indexes loaded from file\n",
      "18.2_subsample_indexes ; loaded from file\n",
      "original size: 31220, new size: 5000 indexes loaded from file\n",
      "4_subsample_indexes ; loaded from file\n",
      "original size: 336286, new size: 5000 indexes loaded from file\n",
      "5_subsample_indexes ; loaded from file\n",
      "original size: 63191, new size: 5000 indexes loaded from file\n",
      "7.1_subsample_indexes ; loaded from file\n",
      "original size: 37175, new size: 5000 indexes loaded from file\n",
      "13_subsample_indexes ; loaded from file\n",
      "original size: 21674, new size: 5000 indexes loaded from file\n",
      "14_subsample_indexes ; loaded from file\n",
      "original size: 49719, new size: 5000 indexes loaded from file\n",
      "15_subsample_indexes ; loaded from file\n",
      "original size: 35205, new size: 5000 indexes loaded from file\n",
      "8.1_subsample_indexes ; loaded from file\n",
      "original size: 106075, new size: 5000 indexes loaded from file\n",
      "17_subsample_indexes ; loaded from file\n",
      "original size: 32521, new size: 5000 indexes loaded from file\n",
      "18_subsample_indexes ; loaded from file\n",
      "original size: 39253, new size: 5000 indexes loaded from file\n",
      "19_subsample_indexes ; loaded from file\n",
      "original size: 55999, new size: 5000 indexes loaded from file\n",
      "20_subsample_indexes ; loaded from file\n",
      "original size: 59748, new size: 5000 indexes loaded from file\n",
      "4.1_subsample_indexes ; loaded from file\n",
      "original size: 99433, new size: 5000 indexes loaded from file\n",
      "7_subsample_indexes ; loaded from file\n",
      "original size: 57086, new size: 5000 indexes loaded from file\n",
      "8_subsample_indexes ; loaded from file\n",
      "original size: 108526, new size: 5000 indexes loaded from file\n",
      "11_subsample_indexes ; loaded from file\n",
      "original size: 33054, new size: 5000 indexes loaded from file\n",
      "['4', '5', '4.1', '7', '8', '11', '7.1', '13', '14', '15', '14.2', '17', '18', '19', '20', '8.1', '18.2', '4f', '5f', '7.1f', '13f', '14f', '15f', '8.1f', '17f', '18f', '19f', '20f', '4.1f', '7f', '8f', '11f']\n"
     ]
    }
   ],
   "source": [
    "for i in k.keys():\n",
    "    \n",
    "\n",
    "    # print(i)\n",
    "    # ind  = int(float(i)) if i!='4.1' and i!='7.1'  else i\n",
    "    # k[i] = subsample_k(k[i].copy(),i,dir_indexes,n=5000).reset_index(drop = True)\n",
    "    \n",
    "    j = i if 'f' not in i else i[:-1] \n",
    "\n",
    "    try:k[i] = subsample_k(k[i].copy(),j,dir_indexes,n=5000)\n",
    "    except: \n",
    "        print(i)\n",
    "        # subsample_k(k[i].copy(),j,dir_indexes,n=5000)\n",
    "    \n",
    "print (list (k.keys()))\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '5', '4.1', '7', '8', '11', '7.1', '13', '14', '15', '14.2', '17', '18', '19', '20', '8.1', '18.2', '4f', '5f', '7.1f', '13f', '14f', '15f', '8.1f', '17f', '18f', '19f', '20f', '4.1f', '7f', '8f', '11f']\n"
     ]
    }
   ],
   "source": [
    "print (list (k.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create batches\n",
    "1. samples were  subsampled to a reasonable size (previous cell)\n",
    "2. every sample is fully inside the batch  - so we know for each index were it came from\n",
    "    1. (the index exists both in the batch and in the sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# uncommon features between samples and samples index to append\n",
    "# from usefull_functions import *\n",
    "b2345 = ['p53', 'ZEB1','NCad','MBD', 'CyclinB1', 'pRB' ];\n",
    "b2345ind = [ '4', '5', '4.1', '7', '8', '11',\n",
    "            '7.1', '13','14','15',\n",
    "            '8.1','17','18','19','20']\n",
    "b2345ind = [f'{i}f' for i in b2345ind]\n",
    "k['b2345'],names['b2345']= createAppendDataset(k.copy(),names['all'].copy(),b2345ind,b2345 )\n",
    "\n",
    "\n",
    "# b123 = ['NCad','ECad','panKeratin',  'p53', 'ZEB1'];b123ind = ['1', '2', '4', '5', '4.1', '7', '8', '11'] problematic since 1,2 with less than 5000samples\n",
    "# b3 = ['p53', 'ZEB1','MBD', 'CyclinB1', 'pRB' ];b3ind = [  '4.1', '7', '8', '11']\n",
    "# b2 = ['p53', 'ZEB1','MBD', 'CyclinB1', 'pRB'];b2ind = [  '4', '5']\n",
    "# b4 = ['p53', 'ZEB1','NCad', 'CyclinB1', 'pRB' ];b4ind = [  '7.3', '13.3','14.3','15.3']\n",
    "# b5 = ['p53', 'ZEB1','NCad', 'CyclinB1', 'pRB' ];b5ind = [  '8.3', '17.3','18.3','19.3','20.3']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove the uncommonFeatures from the  mutual list, create data-dict without uncommonFeatures and append into single dataset\n",
    "\n",
    "# k['b2345'],names['b2345']= createAppendDataset(k.copy(),names['all'].copy(),b2345ind,b2345 )\n",
    "# k['b5'],names['b5']= createAppendDataset(k.copy(),names['all'].copy(),b5ind,b5 )\n",
    "\n",
    "# k['b4'],names['b4']= createAppendDataset(k.copy(),names['all'].copy(),b4ind,b4 )\n",
    "# k['b3'],names['b3']= createAppendDataset(k.copy(),names['all'].copy(),b3ind,b3 )\n",
    "# k['b2'],names['b2']= createAppendDataset(k.copy(),names['all'].copy(),b2ind,b2 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # unitest\n",
    "# K = k['b234'].copy()\n",
    "# for i in np.unique(K['samp']):\n",
    "#     i = str(int(i)) if i!=4.1 and i != 7.1 else str(float(i))\n",
    "\n",
    "#     k_i = k[i].copy()\n",
    "#     k_i_batch = K[K.samp == float(i)].copy()\n",
    "    \n",
    "#     tmp = [i for i in list(k_i.ind) if i not in list(k_i_batch.ind)] + [i for i in list(k_i_batch.ind) if i not in list(k_i.ind)]\n",
    "#     print(i, len(k_i.ind),len(k_i_batch.ind),len(tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k.keys()\n",
    "# k = kk.copy()\n",
    "# names = names_.copy()\n",
    "# kk = k.copy()\n",
    "# names_ = names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names_ = names.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4a_indexes ; loaded from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size: 5000, new size: 4980 indexes loaded from file\n",
      "5a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4714 indexes loaded from file\n",
      "4.1a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4964 indexes loaded from file\n",
      "7a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4582 indexes loaded from file\n",
      "8a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4646 indexes loaded from file\n",
      "11a_indexes ; loaded from file\n",
      "original size: 5000, new size: 3492 indexes loaded from file\n",
      "7.1a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4554 indexes loaded from file\n",
      "13a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4440 indexes loaded from file\n",
      "14a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4503 indexes loaded from file\n",
      "15a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4626 indexes loaded from file\n",
      "14.2a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4489 indexes loaded from file\n",
      "17a_indexes ; loaded from file\n",
      "original size: 5000, new size: 3783 indexes loaded from file\n",
      "18a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4567 indexes loaded from file\n",
      "19a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4007 indexes loaded from file\n",
      "20a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4754 indexes loaded from file\n",
      "8.1a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4791 indexes loaded from file\n",
      "18.2a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4549 indexes loaded from file\n",
      "4a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4980 indexes loaded from file\n",
      "5a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4714 indexes loaded from file\n",
      "7.1a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4554 indexes loaded from file\n",
      "13a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4440 indexes loaded from file\n",
      "14a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4503 indexes loaded from file\n",
      "15a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4626 indexes loaded from file\n",
      "8.1a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4791 indexes loaded from file\n",
      "17a_indexes ; loaded from file\n",
      "original size: 5000, new size: 3783 indexes loaded from file\n",
      "18a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4567 indexes loaded from file\n",
      "19a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4007 indexes loaded from file\n",
      "20a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4754 indexes loaded from file\n",
      "4.1a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4964 indexes loaded from file\n",
      "7a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4582 indexes loaded from file\n",
      "8a_indexes ; loaded from file\n",
      "original size: 5000, new size: 4646 indexes loaded from file\n",
      "11a_indexes ; loaded from file\n",
      "original size: 5000, new size: 3492 indexes loaded from file\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# adjusted = False\n",
    "if adjusted:\n",
    "    existing_keys = [i for i in k.keys() if os.path.exists(f\"{dir_indexes}{i}a_indexes.p\")]\n",
    "    existing_keys = [i for i in k.keys() if (i in existing_keys)or('f' in i and i[:-1] in existing_keys)]\n",
    "    existing_keys.remove('b2345')#doesnt work for batch with multiple samples of same index\n",
    "    for i in existing_keys:\n",
    "        # if 'f' not in i:\n",
    "        #     continue\n",
    "        i_adj = f'{i}a' if 'f' not in i else f'{i[:-1] }af'  \n",
    "        \n",
    "        \n",
    "\n",
    "        K = k[i].copy()\n",
    "        Len = len(k[i])\n",
    "        # print(i_adj)\n",
    "        # k[i_adj]= subsample_k(k[i].copy(),i_adj,dir_indexes)\n",
    "        names[i_adj] = names[i].copy()\n",
    "        del k[i];del names[i]\n",
    "\n",
    "        idx = pickle_load(f\"{i_adj if 'f' not in i_adj else i_adj[:-1]}_indexes\",dir_indexes)\n",
    "        \n",
    "        newIdx = [ K.index[K['ind']==i][0] for i in idx ]\n",
    "        k[i_adj]=K.loc[newIdx].copy()\n",
    "        print (f'original size: {Len}, new size: {len(k[i_adj])} indexes loaded from file')\n",
    "        # del k[i];del names[i]\n",
    "    \n",
    "\n",
    "    b2345 = ['p53', 'ZEB1','NCad','MBD', 'CyclinB1', 'pRB' ];b2345a_ind = [f'{i[:-1]}af' for i in b2345ind]\n",
    "    k['b2345a'],names['b2345a']= createAppendDataset(k.copy(),names['all'].copy(),b2345a_ind,b2345 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fields to remove from df: ['MBD']\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "from usefull_functions import *\n",
    "\n",
    "b45 = ['NCad',      'p53', 'ZEB1', 'CyclinB1', 'pRB' ,  ];b45a_ind = [i[:-1] for i in b2345a_ind][5:]\n",
    "k['b45a'],names['b45a'] = createAppendDataset(k.copy(),names['all'].copy(),b45a_ind,b45 )\n",
    "\n",
    "# print(b45a_ind)    # k['b2345a'],names['b2345a']= createAppendDataset(k.copy(),names['all'].copy(),b2345a_ind,b2345 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b2345', '4a', '5a', '4.1a', '7a', '8a', '11a', '7.1a', '13a', '14a', '15a', '14.2a', '17a', '18a', '19a', '20a', '8.1a', '18.2a', 'b2345a', 'b45a']\n"
     ]
    }
   ],
   "source": [
    "keys = list(k .keys()).copy()\n",
    "for i in keys:\n",
    "    if 'f' not in i :\n",
    "        continue\n",
    "    del k[i];del names[i]\n",
    "print(list(k.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in k.keys():\n",
    "#     if 'MBD' in k[i].columns:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # b2345a = ['p53', 'ZEB1','NCad','MBD', 'CyclinB1', 'pRB' ];b2345a_ind = [ '4a', '5a', '4.1a', '7a', '8a', '11a',\n",
    "    #                                                         '7.1a', '13a','14a','15a']\n",
    "\n",
    "    # # # b123 = ['NCad','ECad','panKeratin',  'p53', 'ZEB1'];b123ind = ['1', '2', '4', '5', '4.1', '7', '8', '11'] problematic since 1,2 with less than 5000samples\n",
    "    # # b3 = ['p53', 'ZEB1','MBD', 'CyclinB1', 'pRB' ];b3ind = [  '4.1', '7', '8', '11']\n",
    "    # # b2 = ['p53', 'ZEB1','MBD', 'CyclinB1', 'pRB'];b2ind = [  '4', '5']\n",
    "    # # b4 = ['p53', 'ZEB1','NCad', 'CyclinB1', 'pRB' ];b4ind = [  '7.3', '13.3','14.3','15.3']\n",
    "\n",
    "\n",
    "    # # remove the uncommonFeatures from the  mutual list, create data-dict without uncommonFeatures and append into single dataset\n",
    "    # print('loaded b234a')\n",
    "    # k['b2345a'],names['b2345a'] = createAppendDataset(k.copy(),names['all'].copy(),b2345a_ind,b2345a )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save pre-scaled data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, K in k.items():\n",
    "#     try:\n",
    "#       del dict;dict ={}\n",
    "#     except:\n",
    "#         dict ={}\n",
    "#     dict['k'] = K\n",
    "#     pickle_dump('k'+i+'_prescaled_dict', dict,dir_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale data \n",
    "\n",
    "using mean, std (whiten data) - achieve std close to 1 mean close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:21:03.888212Z",
     "start_time": "2022-12-11T11:21:03.847807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kb2345 scaled\n",
      "k4a scaled\n",
      "k5a scaled\n",
      "k4.1a scaled\n",
      "k7a scaled\n",
      "k8a scaled\n",
      "k11a scaled\n",
      "k7.1a scaled\n",
      "k13a scaled\n",
      "k14a scaled\n",
      "k15a scaled\n",
      "k14.2a scaled\n",
      "k17a scaled\n",
      "k18a scaled\n",
      "k19a scaled\n",
      "k20a scaled\n",
      "k8.1a scaled\n",
      "k18.2a scaled\n",
      "kb2345a scaled\n",
      "kb45a scaled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, K in k.items():\n",
    "\n",
    "    cols = names[i]['NamesAll']\n",
    "    \n",
    "    k[i][cols] = scale_data(K.copy(),cols)\n",
    "    print (f'k{i} scaled')\n",
    "    # print(K.std().to_frame().T)\n",
    "# the index numbers are unfortunately passes into scaling and norm\n",
    "# after scaling rturn the unscled index numbers\n",
    "\n",
    "# for i, K in k.items():\n",
    "#     k[i]['by_sample'] = names[i]['by_sample']\n",
    "#     k[i]['Ind'] = names[i]['Ind']\n",
    "#     del  names[i]['by_sample'];del names[i]['Ind'] \n",
    "#     # names[i]['by_sample'] = None;names[i]['Ind'] = None\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T11:21:10.287898Z",
     "start_time": "2022-12-11T11:21:07.437490Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if visualize:\n",
    "    plot_hist(k,names['all']['NamesAll'],names['figures'],settings,\n",
    "        func = sns.kdeplot,title = 'Normalized + Scaled',Figname = '2_Hist_normalized_scaled_' )     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final\n",
    "1. Save data to file (pickle)\n",
    "2. drop sample 3\n",
    "3. convert images to pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kb2345; samples = 75000, fetures =  37; saved to file\n",
      "k4a; samples = 4980, fetures =  38; saved to file\n",
      "k5a; samples = 4714, fetures =  38; saved to file\n",
      "k4.1a; samples = 4964, fetures =  37; saved to file\n",
      "k7a; samples = 4582, fetures =  37; saved to file\n",
      "k8a; samples = 4646, fetures =  37; saved to file\n",
      "k11a; samples = 3492, fetures =  37; saved to file\n",
      "k7.1a; samples = 4554, fetures =  38; saved to file\n",
      "k13a; samples = 4440, fetures =  38; saved to file\n",
      "k14a; samples = 4503, fetures =  38; saved to file\n",
      "k15a; samples = 4626, fetures =  38; saved to file\n",
      "k14.2a; samples = 4489, fetures =  38; saved to file\n",
      "k17a; samples = 3783, fetures =  38; saved to file\n",
      "k18a; samples = 4567, fetures =  38; saved to file\n",
      "k19a; samples = 4007, fetures =  38; saved to file\n",
      "k20a; samples = 4754, fetures =  38; saved to file\n",
      "k8.1a; samples = 4791, fetures =  38; saved to file\n",
      "k18.2a; samples = 4549, fetures =  39; saved to file\n",
      "kb2345a; samples = 67403, fetures =  37; saved to file\n",
      "kb45a; samples = 43517, fetures =  38; saved to file\n",
      "total run time = 120.5020889\n"
     ]
    }
   ],
   "source": [
    "# for i, K in k.items():\n",
    "  \n",
    "#   print(len(list(K.columns)))\n",
    "#   print(len(names[i]['NamesAll']))\n",
    "\n",
    "kInd = list(k.keys());\n",
    "if '3'in kInd:kInd.remove('3')\n",
    "for i in kInd:\n",
    "    dict = None;dict ={}\n",
    "    dict = names[i]\n",
    "    pickle_dump('k'+i+'_names_dict', dict,dir_data)\n",
    "\n",
    "    dict = None;dict ={}\n",
    "    K = k[i].copy()\\\n",
    "            .reset_index(drop = True)\n",
    "    # if (K.index == range(len(K))).all():\n",
    "    dict['k'] = K\n",
    "    pickle_dump(f'k{i}_dict', dict,dir_data)\n",
    "    print (f'k{i}; samples = {len(K)}, fetures =  {len(K.columns)}; saved to file')\n",
    "\n",
    "end = time.process_time()\n",
    "print ('total run time =', end-start )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0 3492\n",
      "7.1 4554\n",
      "13.0 4440\n",
      "14.0 4503\n",
      "15.0 4626\n",
      "8.1 4791\n",
      "17.0 3783\n",
      "18.0 4567\n",
      "19.0 4007\n",
      "20.0 4754\n",
      "Index(['H4', 'H3', 'H3.3', 'ECad', 'panKeratin', 'K5', 'EpCam', 'H3K27me2',\n",
      "       'EZH2', 'gH2AX', 'aSMA', 'H3K36me2', 'H3K4me1', 'H3K9me2', 'H4K16ac',\n",
      "       'H2Aub', 'Vimentin', 'H3K64ac', 'BMI-1', 'H3K27ac', 'H4K20me3', 'ER',\n",
      "       'CD49f', 'CD24', 'GATA3', 'H3K9ac', 'H3K9me3', 'CD44', 'Ki67', 'K8-18',\n",
      "       'H3K36me3', 'H3K4me3', 'H3K27me3', 'H3S28p', 'samp', 'ind', 'CD45',\n",
      "       'MBD'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "K = k['b45a'].copy()\n",
    "existing_keys = K['samp'].unique().tolist()\n",
    "    # # keys.remove('b2345')\n",
    "for i in existing_keys:\n",
    "    print(i,len(K[K['samp']==i]))\n",
    "\n",
    "print (K.keys())\n",
    "# # 4.0 4979\n",
    "# # 5.0 4714\n",
    "# # 4.1 4964\n",
    "# # 7.0 4583\n",
    "# # 8.0 4646\n",
    "# # 11.0 3493\n",
    "# # 7.1 4555\n",
    "# # 13.0 4414\n",
    "# # 14.0 4505\n",
    "# # 15.0 4626\n",
    "# # 8.1 4791\n",
    "# # 17.0 3782\n",
    "# # 18.0 4570\n",
    "# # 19.0 4004\n",
    "# # 20.0 4755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "25238166783bd3667da868d9e05a785eda3f5ca355f77ce9c051267fa7411fd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
